# -*- coding: utf-8 -*-
"""categorizing_financial_health_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18wItXfrlZiaNWTJ5lBYl2PyooFm-AA6a

# **Import Library**
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# **Loading Dataset**

"""

from google.colab import files

# Mengupload file UMR_PROPINSI_2019-2020.csv
uploaded = files.upload()

df = pd.read_csv("financial_data_indonesia.csv")

# Sample preview of the data
df.head()

"""## **EDA**"""

df.info()

df.describe()

df.isnull().all()

"""# **Preparing Data**

**Encoding Category Column**
"""

from sklearn.preprocessing import LabelEncoder

# Create separate LabelEncoder objects for 'location' and 'financial_status'
label_encoder_location = LabelEncoder()
label_encoder_financial_status = LabelEncoder()

# Encode the 'location' column using the label encoder
df['location'] = label_encoder_location.fit_transform(df['location'])

# Encode the 'financial_status' column using the label encoder
df['financial_status'] = label_encoder_financial_status.fit_transform(df['financial_status'])

# Display the encoded class labels for 'location' and 'financial_status'
print("Classes for 'location':", label_encoder_location.classes_)
print("Classes for 'financial_status':", label_encoder_financial_status.classes_)

# Display the first few rows of the dataframe after encoding
print(df.head())

"""**Split Data**"""

# split data
from sklearn.model_selection import train_test_split
features = ['location', 'age', 'income', 'food_expenses', 'transport_expenses', 'housing_cost',
                     'water_bill', 'electricity_bill', 'internet_cost', 'debt', 'savings']

X = df[features]
y = df['financial_status']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""**Scaling Some Column**"""

from sklearn.preprocessing import StandardScaler

# Create an instance of StandardScaler for feature scaling
scaler = StandardScaler()

# List of feature names to be scaled
features_to_scale = ['income', 'food_expenses', 'transport_expenses',
                     'housing_cost', 'water_bill', 'electricity_bill',
                     'internet_cost', 'debt', 'savings']

# Apply scaling to the selected features in the training set
X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])

# Apply the same scaling to the selected features in the validation set
X_val[features_to_scale] = scaler.transform(X_val[features_to_scale])

# Apply the same scaling to the selected features in the test set
X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])

# Display the first few rows of the scaled training data
print(X_train.head())

# Cek distribusi kelas di y
print(y.value_counts())

# Cek distribusi kelas di y_train
print(y_train.value_counts())

"""**Oversampling**"""

from imblearn.over_sampling import RandomOverSampler

# Create an instance of RandomOverSampler to handle class imbalance by oversampling the minority class
oversample = RandomOverSampler(random_state=42)

# Apply oversampling to the training data (X_train, y_train)
X_train_oversampled, y_train_oversampled = oversample.fit_resample(X_train, y_train)

# Update X_train and y_train with the oversampled data
X_train, y_train = X_train_oversampled, y_train_oversampled

# Check the class distribution in y_train after applying oversampling
print(pd.Series(y_train).value_counts())

"""# **Modelling**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping

# EarlyStopping callback to stop training when the validation loss stops improving
early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    patience=10,  # Number of epochs with no improvement before stopping
    restore_best_weights=True,  # Restore the weights from the best epoch
)

# Function to build the neural network model
def build_model(input_shape):
    model = Sequential([
        # First hidden layer with L2 regularization and ReLU activation
        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),
        Dropout(0.2),  # Dropout layer to prevent overfitting

        # Second hidden layer with L2 regularization and ReLU activation
        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.2),  # Dropout layer to prevent overfitting

        # Third hidden layer with L2 regularization and ReLU activation
        Dense(16, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.2),  # Dropout layer to prevent overfitting

        # Output layer with 3 neurons for multi-class classification
        Dense(3, activation='linear')
    ])
    return model

# Function to compile and train the model
def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
    # Define loss function for multi-class classification
    loss_function = SparseCategoricalCrossentropy(from_logits=True)

    # Compile the model with Adam optimizer and sparse categorical cross-entropy loss
    model.compile(optimizer=Adam(learning_rate=0.00001),
                  loss=loss_function,
                  metrics=['accuracy'])

    # Train the model with early stopping to prevent overfitting
    history = model.fit(X_train, y_train,
                        epochs=epochs,  # Number of epochs to train
                        batch_size=batch_size,  # Batch size
                        validation_data=(X_val, y_val),  # Validation data to evaluate the model
                        callbacks=[early_stopping])  # Early stopping callback
    return history

# Get the input shape based on training data
input_shape = X_train.shape[1]

# Build the model
model = build_model(input_shape)

# Train the model with the training and validation data
history = train_model(model, X_train, y_train, X_val, y_val)

# Evaluate the trained model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy}")

# Save the trained model in two formats for later use
model.save("categorizing_financial_health_model.h5")
model.save('categorizing_financial_health_model.keras')

"""# **Evaluation**"""

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Evaluate the model on the test data
y_pred = model.predict(X_test)

# Convert predicted probabilities to predicted class labels
y_pred_class = np.argmax(y_pred, axis=1)  # Convert the prediction results into the predicted class labels

# Display the classification report, which includes precision, recall, and F1-score for each class
print("Classification Report:")
print(classification_report(y_test, y_pred_class, target_names=['Sehat', 'Kurang Sehat', 'Tidak Sehat']))

# Display the confusion matrix, which shows the number of correct and incorrect predictions
print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred_class)
print(conf_matrix)

# Visualize the confusion matrix as a heatmap for better interpretation
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Sehat', 'Kurang Sehat', 'Tidak Sehat'], yticklabels=['Sehat', 'Kurang Sehat', 'Tidak Sehat'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **Test the model with new input of features**"""

from tensorflow.keras.models import load_model
from tensorflow.nn import softmax

# Load the pre-trained model
model = load_model("categorizing_financial_health_model.h5")

# Initialize the LabelEncoder for 'location' and set its classes based on the training data
label_encoder_location = LabelEncoder()
label_encoder_location.classes_ = np.array(['ACEH', 'BALI', 'BANTEN', 'BENGKULU', 'DI YOGYAKARTA', 'DKI JAKARTA',
 'GORONTALO', 'JAMBI', 'JAWA BARAT', 'JAWA TENGAH', 'JAWA TIMUR',
 'KALIMANTAN BARAT', 'KALIMANTAN SELATAN', 'KALIMANTAN TENGAH',
 'KALIMANTAN TIMUR', 'KALIMANTAN UTARA', 'KEP. BANGKA BELITUNG', 'KEP. RIAU',
 'LAMPUNG', 'MALUKU', 'MALUKU UTARA', 'NUSA TENGGARA BARAT',
 'NUSA TENGGARA TIMUR', 'PAPUA', 'PAPUA BARAT', 'RIAU', 'SULAWESI BARAT',
 'SULAWESI SELATAN', 'SULAWESI TENGAH', 'SULAWESI TENGGARA', 'SULAWESI UTARA',
 'SUMATERA BARAT', 'SUMATERA SELATAN', 'SUMATERA UTARA'])  # Define actual classes from training data

# Initialize the LabelEncoder for 'financial_status' and set its classes based on the training data
label_encoder_financial_status = LabelEncoder()
label_encoder_financial_status.classes_ = np.array(["Kurang Sehat", "Sehat", "Tidak Sehat"])  # Define actual classes from training data

# Define the feature names for user input
feature_names = [
    "location", "age", "income", "food_expenses", "transport_expenses", "housing_cost",
    "water_bill", "electricity_bill", "internet_cost", "debt", "savings"
]

def get_user_input(feature_names):
    """
    Prompts the user to input values for each feature and returns them as a numpy array.
    The function handles both categorical (location and financial status) and numerical features.
    """
    user_data = []
    for feature in feature_names:
        if feature == 'location':  # For the 'location' column, use LabelEncoder
            value = input(f"Please enter the location ({', '.join(label_encoder_location.classes_)}): ")
            value = label_encoder_location.transform([value])[0]  # Convert the input into an encoded label
        elif feature == 'financial_status':  # For the 'financial_status' column, use LabelEncoder
            value = input(f"Please enter the financial status ({', '.join(label_encoder_financial_status.classes_)}): ")
            value = label_encoder_financial_status.transform([value])[0]  # Convert the input into an encoded label
        else:  # For all other features, take numeric input
            value = float(input(f"Please enter the value for {feature}: "))
        user_data.append(value)
    return np.array([user_data])  # Return as a numpy array for model input

def predict_financial_health(model, user_input):
    """
    Makes a prediction on the financial health based on user input.
    Converts logits to probabilities using softmax, and returns the predicted financial health category.
    """
    logits = model.predict(user_input)
    # Convert logits to probabilities using softmax
    probabilities = softmax(logits).numpy()
    # Get the class with the highest probability
    predicted_class = np.argmax(probabilities, axis=1)
    class_labels = ["Kurang Sehat", "Sehat", "Tidak Sehat"]
    return class_labels[predicted_class[0]]  # Return the predicted financial health label

# Get user input for each feature
user_input = get_user_input(feature_names)

# Make a prediction and display the result
predicted_health = predict_financial_health(model, user_input)
print(f"The predicted financial health is: {predicted_health}")
